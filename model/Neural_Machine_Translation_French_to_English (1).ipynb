{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Seq2Seq\n",
        "\n",
        "Seq2Seq model is the learning of sequence text as input and sequence text in output. Note that the length of the input and output must not be the same. A typical sequence to sequence model consiste of encoder and decoder which are themselves two separate neural networks combined into a single giant network. Both encoder and decoder are typically LSTM or GRU models.\n",
        "Some applications of Seq2Seq models are Neural machine translation, Image captioning, speech recognition, chat-bot, time-series forecasting e.t.c."
      ],
      "metadata": {
        "id": "CY0GVu0BXi-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Translation using seq2seq with Attention Impletmentation\n",
        "In this project we are going to learn seq2seq with which we implement attention mechanism for better result. The problem with typical seq2seq model which has only encoder and decoder is that it is not better for long sentences and there are some issue with performance also. That why we add attention mechanism for better performance and fast training.\n",
        "Steps:\n",
        "1) Loading Data\n",
        "2) Preprocessing\n",
        "3) Tokenization\n",
        "4) Padding\n",
        "5) Train Test Splitting\n",
        "4) Encoder\n",
        "7) Bahdanau Attention\n",
        "8) Decoder\n",
        "9) Traing\n",
        "10) Inference\n",
        "11) Evaluation"
      ],
      "metadata": {
        "id": "AAMEsC-lYjYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data\n",
        "we use french to english dataset for this project. We download this dataset from http://www.manythings.org/anki/.\n",
        "In this dataset every line is the paire of first english and then france sentence. We convert english and french sentence separately we do data cleaning."
      ],
      "metadata": {
        "id": "JTS96_V7Z7mg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhyMvVRtXTuY",
        "outputId": "48f4e100-6266-44a1-8bc9-d67abccb8560"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentence pairs loaded: 239189\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "\n",
        "file_path = '/content/fra.txt'\n",
        "\n",
        "pairs = []\n",
        "\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        parts = line.split('\\t')\n",
        "\n",
        "        if len(parts) >= 2:\n",
        "            english = parts[0].strip()\n",
        "            french = parts[1].strip()\n",
        "            pairs.append((english, french))\n",
        "\n",
        "print(\"Total sentence pairs loaded:\", len(pairs))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"English: {pairs[0][0]}\")\n",
        "print(f\"French: {pairs[0][1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8RxXGjNcT2F",
        "outputId": "1ca824a4-6d4b-4b07-90c1-17bb15ec88be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: Go.\n",
            "French: Va !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we done with data loading lets move to data preprocessing which is neccessary for every nlp task. Becuase model does not know text. It knows only number so first we do data cleaning we remove some extra words or tokens which are not neccessary for our model"
      ],
      "metadata": {
        "id": "2fuixdIZdOSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "  \"\"\"convert special french character like √©, √®, √™ to normal form.\"\"\"\n",
        "  return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )"
      ],
      "metadata": {
        "id": "9k9duRENdGBn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(s):\n",
        "  s = s.lower().strip()\n",
        "  s = unicode_to_ascii(s) # normalize unicode\n",
        "  s = re.sub(r\"([?.!,¬ø])\", r\" \\1 \", s) # separate punctuation\n",
        "  s = re.sub(r'\\s+', ' ', s) # remove extra spaces\n",
        "  s = re.sub(r\"[^a-zA-Z?.!,¬ø']+\", \" \", s) # need only letters number and some other punctuation\n",
        "  return s.strip()"
      ],
      "metadata": {
        "id": "TJ9onWImePL6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_pairs = []\n",
        "for eng, fr in pairs:\n",
        "    eng_clean = clean_text(eng)\n",
        "    fr_clean = clean_text(fr)\n",
        "    eng_clean = '<start> ' + eng_clean + ' <end>'  # we add some spatial word start and end in the fre_clean data\n",
        "    clean_pairs.append((eng_clean, fr_clean))"
      ],
      "metadata": {
        "id": "1dM3FccHfGGG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(f\"English: {clean_pairs[i][0]}\")\n",
        "  print(f\"French: {clean_pairs[i][1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCkIaJjlf5-k",
        "outputId": "22f5e735-fed4-4192-81da-f2a8930d20df"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: <start> go . <end>\n",
            "French: va !\n",
            "English: <start> go . <end>\n",
            "French: marche .\n",
            "English: <start> go . <end>\n",
            "French: en route !\n",
            "English: <start> go . <end>\n",
            "French: bouge !\n",
            "English: <start> hi . <end>\n",
            "French: salut !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_eng_words = set()\n",
        "for en, fr in clean_pairs:\n",
        "    for word in en.split():\n",
        "        all_eng_words.add(word)\n",
        "\n",
        "print(\"Unique English words:\", len(all_eng_words))"
      ],
      "metadata": {
        "id": "-bK84wjwgldT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577b60c5-8aa8-486d-a7b2-5a5278ee6d51"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique English words: 16850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we done with some preprocessing but some are still exist now  we tokenized our data by some length of unique words. mean we add top unique words and remaining we adACd to unk. Mean in our test if words is out of vocabulary it assign it to unk if add all length of unique words our training time and memore time will increased.\n",
        "Instead of using 16000 we only use 10000 and rest of words are goes to unk."
      ],
      "metadata": {
        "id": "hNcqd96RpgdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "eng_sentences = [eng for eng,fre in clean_pairs]\n",
        "fre_sentences = [fre for eng,fre in clean_pairs]\n",
        "\n",
        "\n",
        "num_words_src = 20000   # English words limit\n",
        "num_words_tgt = 10000   # French words limit\n",
        "\n",
        "eng_tokenizer = Tokenizer(num_words = num_words_tgt, filters = \"\", oov_token = '<unk>')\n",
        "eng_tokenizer.fit_on_texts(eng_sentences)\n",
        "\n",
        "fr_tokenizer = Tokenizer(num_words = num_words_src,filters = \"\",oov_token = '<unk>')\n",
        "fr_tokenizer.fit_on_texts(fre_sentences)\n",
        "\n"
      ],
      "metadata": {
        "id": "rI_NA5uupUxh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"English vocabulary size:\", len(eng_tokenizer.word_index))\n",
        "print(\"French vocabulary size:\", len(fr_tokenizer.word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wssahYzsOYv",
        "outputId": "b25bf946-a996-4462-99da-a5009682790a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocabulary size: 16851\n",
            "French vocabulary size: 29456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "eng_sequences = eng_tokenizer.texts_to_sequences([eng for eng,fre in clean_pairs])\n",
        "fre_sequences = fr_tokenizer.texts_to_sequences([fre for eng,fre in clean_pairs])\n",
        "\n",
        "max_eng_len = max(len(seq) for seq in eng_sequences)\n",
        "max_fr_len = max(len(seq) for seq in fre_sequences)\n",
        "\n",
        "eng_padded = pad_sequences(eng_sequences, maxlen=max_eng_len, padding='post')\n",
        "fr_padded = pad_sequences(fre_sequences, maxlen=max_fr_len, padding='post')\n"
      ],
      "metadata": {
        "id": "Dm8F5mU6sQR-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_eng_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qx8ZZg9VsY_N",
        "outputId": "5470d43c-dfbc-4acd-96b1-b1311f583558"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"English padded shape:\", eng_padded.shape)\n",
        "print(\"French padded shape:\", fr_padded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKY4VRHZtVhZ",
        "outputId": "ad4f3e39-3ca1-4a1d-d37a-6724f0c084fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English padded shape: (239189, 68)\n",
            "French padded shape: (239189, 69)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "y = eng_padded\n",
        "X = fr_padded\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Testing data shape:\", X_test.shape)\n",
        "print(\"Training y data\", y_train.shape)\n",
        "print(\"Testing y data\", y_test.shape)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(BATCH_SIZE)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE)\n",
        "\n",
        "print(\"‚úÖ TensorFlow datasets created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RdMWJcftbUe",
        "outputId": "2e54a857-7334-426f-de58-5c32d819f40e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (191351, 69)\n",
            "Testing data shape: (47838, 69)\n",
            "Training y data (191351, 68)\n",
            "Testing y data (47838, 68)\n",
            "‚úÖ TensorFlow datasets created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder\n"
      ],
      "metadata": {
        "id": "T3MZ7y6YuLLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = LSTM(self.enc_units,return_sequences = True,return_state = True,recurrent_initializer='glorot_uniform')\n",
        "  def call(self,x,hidden):\n",
        "    x = self.embedding(x)\n",
        "    output,state_h,state_c = self.lstm(x,initial_state = hidden)\n",
        "    return output,state_h,state_c\n",
        "  def initial_hidden_state(self):\n",
        "    return (tf.zeros((self.batch_size, self.enc_units)),\n",
        "                tf.zeros((self.batch_size, self.enc_units)))\n"
      ],
      "metadata": {
        "id": "3X8tIzmht4in"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention"
      ],
      "metadata": {
        "id": "CZh-wrOTvrtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self,units):\n",
        "    super(BahdanauAttention,self).__init__()\n",
        "    self.W1 = Dense(units)\n",
        "    self.W2 = Dense(units)\n",
        "    self.V = Dense(1)\n",
        "  def call(self,encoder_outputs,decoder_hidden):\n",
        "    decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
        "    score = self.V(tf.nn.tanh(self.W1(encoder_outputs)+self.W2(decoder_hidden_with_time_axis)))\n",
        "    attention_weights = tf.nn.softmax(score,axis = 1)\n",
        "    context_vector = attention_weights * encoder_outputs\n",
        "    context_vector = tf.reduce_sum(context_vector,axis = 1)\n",
        "    return context_vector, tf.squeeze(attention_weights,axis = -1)"
      ],
      "metadata": {
        "id": "ufmG7qiivpaa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "B97HLatFzcwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self,vocab_size,embedding_dim,dec_units,batch_size):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.enc_units = dec_units\n",
        "    self.embedding = Embedding(vocab_size,embedding_dim)\n",
        "    self.lstm = LSTM(self.enc_units,return_sequences = True,return_state = True,recurrent_initializer = 'glorot_uniform')\n",
        "    self.fc = Dense(vocab_size)\n",
        "    self.attention = BahdanauAttention(self.enc_units)\n",
        "  def call(self,x,hidden,encoder_outputs):\n",
        "    if isinstance(hidden, tuple):\n",
        "        hidden = hidden[0]\n",
        "    context_vector,attention_weights = self.attention(encoder_outputs,hidden)\n",
        "    x = self.embedding(x)\n",
        "    x = tf.concat([tf.expand_dims(context_vector,1),x],axis = -1)\n",
        "    output,state_h,state_c = self.lstm(x)\n",
        "    output = tf.reshape(output,(-1,output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "    return x,(state_h,state_c),attention_weights"
      ],
      "metadata": {
        "id": "nVReWwMjzo9g"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CxlUkw9fac8S"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = X_train.shape[0]\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_inp_size = len(fr_tokenizer.word_index) + 1\n",
        "vocab_tar_size = len(eng_tokenizer.word_index) + 1\n",
        "\n",
        "# Create Encoder and Decoder objects\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "IltxaUZW1lif"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_inp_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_htdXMW_Sp_s",
        "outputId": "196e8c69-90f4-434c-df9d-b31f02f27106"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29457"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_tar_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDvvhZzCOsaj",
        "outputId": "1fd4ebbb-58e5-4986-b432-4331520bec59"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16852"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer and loss\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none'\n",
        ")\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))  # ignore padding\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "0g7TooFm2wcp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why Masking\n",
        "If we do not do masking, model try to predict padded token that why training become slow and inaccurate.\n",
        "loss artificially become so high.\n",
        "\n",
        "With Masking model focuss only on meaningful tokens\n",
        "convergence become faster and translation quality improves"
      ],
      "metadata": {
        "id": "a1xMRIt356DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Encoder pass\n",
        "        enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = (enc_h,enc_c)\n",
        "\n",
        "        # Decoder input starts with <start> token\n",
        "        dec_input = tf.expand_dims([eng_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing: feeding the target as next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # use true label (teacher forcing)\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = loss / int(targ.shape[1])\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n"
      ],
      "metadata": {
        "id": "XmvKZIwy3fo9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 8\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    enc_hidden = encoder.initial_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(train_ds.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "\n",
        "    # Average loss per epoch\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss / steps_per_epoch:.4f}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsjQvTtRWXkP",
        "outputId": "cfd69ec1-eb03-4aab-a687-df214f266cbd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 1.1181\n",
            "Epoch 1 Batch 100 Loss 0.6277\n",
            "Epoch 1 Batch 200 Loss 0.5482\n",
            "Epoch 1 Batch 300 Loss 0.5418\n",
            "Epoch 1 Batch 400 Loss 0.5190\n",
            "Epoch 1 Batch 500 Loss 0.4711\n",
            "Epoch 1 Batch 600 Loss 0.4753\n",
            "Epoch 1 Batch 700 Loss 0.5055\n",
            "Epoch 1 Batch 800 Loss 0.4597\n",
            "Epoch 1 Batch 900 Loss 0.4469\n",
            "Epoch 1 Batch 1000 Loss 0.4103\n",
            "Epoch 1 Batch 1100 Loss 0.4039\n",
            "Epoch 1 Batch 1200 Loss 0.3656\n",
            "Epoch 1 Batch 1300 Loss 0.3753\n",
            "Epoch 1 Batch 1400 Loss 0.4029\n",
            "Epoch 1 Batch 1500 Loss 0.3587\n",
            "Epoch 1 Batch 1600 Loss 0.3147\n",
            "Epoch 1 Batch 1700 Loss 0.3500\n",
            "Epoch 1 Batch 1800 Loss 0.3187\n",
            "Epoch 1 Batch 1900 Loss 0.3081\n",
            "Epoch 1 Batch 2000 Loss 0.2996\n",
            "Epoch 1 Batch 2100 Loss 0.3361\n",
            "Epoch 1 Batch 2200 Loss 0.3174\n",
            "Epoch 1 Batch 2300 Loss 0.2896\n",
            "Epoch 1 Batch 2400 Loss 0.3402\n",
            "Epoch 1 Batch 2500 Loss 0.3022\n",
            "Epoch 1 Batch 2600 Loss 0.2697\n",
            "Epoch 1 Batch 2700 Loss 0.2939\n",
            "Epoch 1 Batch 2800 Loss 0.2921\n",
            "Epoch 1 Batch 2900 Loss 0.2316\n",
            "Epoch 1 Loss 0.3920\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.2601\n",
            "Epoch 2 Batch 100 Loss 0.2164\n",
            "Epoch 2 Batch 200 Loss 0.2382\n",
            "Epoch 2 Batch 300 Loss 0.2082\n",
            "Epoch 2 Batch 400 Loss 0.2249\n",
            "Epoch 2 Batch 500 Loss 0.2931\n",
            "Epoch 2 Batch 600 Loss 0.2092\n",
            "Epoch 2 Batch 700 Loss 0.2213\n",
            "Epoch 2 Batch 800 Loss 0.1900\n",
            "Epoch 2 Batch 900 Loss 0.2318\n",
            "Epoch 2 Batch 1000 Loss 0.2211\n",
            "Epoch 2 Batch 1100 Loss 0.1770\n",
            "Epoch 2 Batch 1200 Loss 0.1843\n",
            "Epoch 2 Batch 1300 Loss 0.1919\n",
            "Epoch 2 Batch 1400 Loss 0.2003\n",
            "Epoch 2 Batch 1500 Loss 0.1462\n",
            "Epoch 2 Batch 1600 Loss 0.2007\n",
            "Epoch 2 Batch 1700 Loss 0.1817\n",
            "Epoch 2 Batch 1800 Loss 0.2339\n",
            "Epoch 2 Batch 1900 Loss 0.1870\n",
            "Epoch 2 Batch 2000 Loss 0.1335\n",
            "Epoch 2 Batch 2100 Loss 0.1808\n",
            "Epoch 2 Batch 2200 Loss 0.1490\n",
            "Epoch 2 Batch 2300 Loss 0.1504\n",
            "Epoch 2 Batch 2400 Loss 0.1616\n",
            "Epoch 2 Batch 2500 Loss 0.1556\n",
            "Epoch 2 Batch 2600 Loss 0.1352\n",
            "Epoch 2 Batch 2700 Loss 0.1136\n",
            "Epoch 2 Batch 2800 Loss 0.1366\n",
            "Epoch 2 Batch 2900 Loss 0.1378\n",
            "Epoch 2 Loss 0.1905\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.1499\n",
            "Epoch 3 Batch 100 Loss 0.1535\n",
            "Epoch 3 Batch 200 Loss 0.1387\n",
            "Epoch 3 Batch 300 Loss 0.1200\n",
            "Epoch 3 Batch 400 Loss 0.1518\n",
            "Epoch 3 Batch 500 Loss 0.1410\n",
            "Epoch 3 Batch 600 Loss 0.1509\n",
            "Epoch 3 Batch 700 Loss 0.1528\n",
            "Epoch 3 Batch 800 Loss 0.1669\n",
            "Epoch 3 Batch 900 Loss 0.1094\n",
            "Epoch 3 Batch 1000 Loss 0.1505\n",
            "Epoch 3 Batch 1100 Loss 0.1116\n",
            "Epoch 3 Batch 1200 Loss 0.1147\n",
            "Epoch 3 Batch 1300 Loss 0.1363\n",
            "Epoch 3 Batch 1400 Loss 0.1008\n",
            "Epoch 3 Batch 1500 Loss 0.1170\n",
            "Epoch 3 Batch 1600 Loss 0.1123\n",
            "Epoch 3 Batch 1700 Loss 0.1305\n",
            "Epoch 3 Batch 1800 Loss 0.1450\n",
            "Epoch 3 Batch 1900 Loss 0.1052\n",
            "Epoch 3 Batch 2000 Loss 0.1091\n",
            "Epoch 3 Batch 2100 Loss 0.1095\n",
            "Epoch 3 Batch 2200 Loss 0.1203\n",
            "Epoch 3 Batch 2300 Loss 0.1214\n",
            "Epoch 3 Batch 2400 Loss 0.1206\n",
            "Epoch 3 Batch 2500 Loss 0.0944\n",
            "Epoch 3 Batch 2600 Loss 0.0881\n",
            "Epoch 3 Batch 2700 Loss 0.1241\n",
            "Epoch 3 Batch 2800 Loss 0.1479\n",
            "Epoch 3 Batch 2900 Loss 0.1286\n",
            "Epoch 3 Loss 0.1299\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.0908\n",
            "Epoch 4 Batch 100 Loss 0.1006\n",
            "Epoch 4 Batch 200 Loss 0.1206\n",
            "Epoch 4 Batch 300 Loss 0.1267\n",
            "Epoch 4 Batch 400 Loss 0.1121\n",
            "Epoch 4 Batch 500 Loss 0.1200\n",
            "Epoch 4 Batch 600 Loss 0.1013\n",
            "Epoch 4 Batch 700 Loss 0.0983\n",
            "Epoch 4 Batch 800 Loss 0.1159\n",
            "Epoch 4 Batch 900 Loss 0.1105\n",
            "Epoch 4 Batch 1000 Loss 0.0783\n",
            "Epoch 4 Batch 1100 Loss 0.0924\n",
            "Epoch 4 Batch 1200 Loss 0.0846\n",
            "Epoch 4 Batch 1300 Loss 0.1190\n",
            "Epoch 4 Batch 1400 Loss 0.0980\n",
            "Epoch 4 Batch 1500 Loss 0.1127\n",
            "Epoch 4 Batch 1600 Loss 0.1195\n",
            "Epoch 4 Batch 1700 Loss 0.0948\n",
            "Epoch 4 Batch 1800 Loss 0.0870\n",
            "Epoch 4 Batch 1900 Loss 0.1047\n",
            "Epoch 4 Batch 2000 Loss 0.0839\n",
            "Epoch 4 Batch 2100 Loss 0.0987\n",
            "Epoch 4 Batch 2200 Loss 0.1088\n",
            "Epoch 4 Batch 2300 Loss 0.0890\n",
            "Epoch 4 Batch 2400 Loss 0.1119\n",
            "Epoch 4 Batch 2500 Loss 0.0959\n",
            "Epoch 4 Batch 2600 Loss 0.0971\n",
            "Epoch 4 Batch 2700 Loss 0.0971\n",
            "Epoch 4 Batch 2800 Loss 0.0899\n",
            "Epoch 4 Batch 2900 Loss 0.1197\n",
            "Epoch 4 Loss 0.1004\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1130\n",
            "Epoch 5 Batch 100 Loss 0.0896\n",
            "Epoch 5 Batch 200 Loss 0.0940\n",
            "Epoch 5 Batch 300 Loss 0.0910\n",
            "Epoch 5 Batch 400 Loss 0.1029\n",
            "Epoch 5 Batch 500 Loss 0.1015\n",
            "Epoch 5 Batch 600 Loss 0.0813\n",
            "Epoch 5 Batch 700 Loss 0.0673\n",
            "Epoch 5 Batch 800 Loss 0.0693\n",
            "Epoch 5 Batch 900 Loss 0.0804\n",
            "Epoch 5 Batch 1000 Loss 0.0794\n",
            "Epoch 5 Batch 1100 Loss 0.0748\n",
            "Epoch 5 Batch 1200 Loss 0.0845\n",
            "Epoch 5 Batch 1300 Loss 0.0823\n",
            "Epoch 5 Batch 1400 Loss 0.0641\n",
            "Epoch 5 Batch 1500 Loss 0.0909\n",
            "Epoch 5 Batch 1600 Loss 0.0708\n",
            "Epoch 5 Batch 1700 Loss 0.0658\n",
            "Epoch 5 Batch 1800 Loss 0.0947\n",
            "Epoch 5 Batch 1900 Loss 0.0730\n",
            "Epoch 5 Batch 2000 Loss 0.0746\n",
            "Epoch 5 Batch 2100 Loss 0.1022\n",
            "Epoch 5 Batch 2200 Loss 0.0697\n",
            "Epoch 5 Batch 2300 Loss 0.0733\n",
            "Epoch 5 Batch 2400 Loss 0.0971\n",
            "Epoch 5 Batch 2500 Loss 0.0759\n",
            "Epoch 5 Batch 2600 Loss 0.0823\n",
            "Epoch 5 Batch 2700 Loss 0.0725\n",
            "Epoch 5 Batch 2800 Loss 0.0806\n",
            "Epoch 5 Batch 2900 Loss 0.0591\n",
            "Epoch 5 Loss 0.0821\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0884\n",
            "Epoch 6 Batch 100 Loss 0.0830\n",
            "Epoch 6 Batch 200 Loss 0.0819\n",
            "Epoch 6 Batch 300 Loss 0.0626\n",
            "Epoch 6 Batch 400 Loss 0.0784\n",
            "Epoch 6 Batch 500 Loss 0.0936\n",
            "Epoch 6 Batch 600 Loss 0.0677\n",
            "Epoch 6 Batch 700 Loss 0.0677\n",
            "Epoch 6 Batch 800 Loss 0.0775\n",
            "Epoch 6 Batch 900 Loss 0.0635\n",
            "Epoch 6 Batch 1000 Loss 0.0692\n",
            "Epoch 6 Batch 1100 Loss 0.0629\n",
            "Epoch 6 Batch 1200 Loss 0.0806\n",
            "Epoch 6 Batch 1300 Loss 0.0767\n",
            "Epoch 6 Batch 1400 Loss 0.0697\n",
            "Epoch 6 Batch 1500 Loss 0.0473\n",
            "Epoch 6 Batch 1600 Loss 0.0845\n",
            "Epoch 6 Batch 1700 Loss 0.0851\n",
            "Epoch 6 Batch 1800 Loss 0.0725\n",
            "Epoch 6 Batch 1900 Loss 0.0556\n",
            "Epoch 6 Batch 2000 Loss 0.0592\n",
            "Epoch 6 Batch 2100 Loss 0.0948\n",
            "Epoch 6 Batch 2200 Loss 0.0474\n",
            "Epoch 6 Batch 2300 Loss 0.0576\n",
            "Epoch 6 Batch 2400 Loss 0.0691\n",
            "Epoch 6 Batch 2500 Loss 0.0641\n",
            "Epoch 6 Batch 2600 Loss 0.0647\n",
            "Epoch 6 Batch 2700 Loss 0.0621\n",
            "Epoch 6 Batch 2800 Loss 0.0652\n",
            "Epoch 6 Batch 2900 Loss 0.0644\n",
            "Epoch 6 Loss 0.0689\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0674\n",
            "Epoch 7 Batch 100 Loss 0.0592\n",
            "Epoch 7 Batch 200 Loss 0.0656\n",
            "Epoch 7 Batch 300 Loss 0.0483\n",
            "Epoch 7 Batch 400 Loss 0.0659\n",
            "Epoch 7 Batch 500 Loss 0.0541\n",
            "Epoch 7 Batch 600 Loss 0.0700\n",
            "Epoch 7 Batch 700 Loss 0.0605\n",
            "Epoch 7 Batch 800 Loss 0.0639\n",
            "Epoch 7 Batch 900 Loss 0.0517\n",
            "Epoch 7 Batch 1000 Loss 0.0516\n",
            "Epoch 7 Batch 1100 Loss 0.0658\n",
            "Epoch 7 Batch 1200 Loss 0.0674\n",
            "Epoch 7 Batch 1300 Loss 0.0490\n",
            "Epoch 7 Batch 1400 Loss 0.0386\n",
            "Epoch 7 Batch 1500 Loss 0.0643\n",
            "Epoch 7 Batch 1600 Loss 0.0546\n",
            "Epoch 7 Batch 1700 Loss 0.0750\n",
            "Epoch 7 Batch 1800 Loss 0.0633\n",
            "Epoch 7 Batch 1900 Loss 0.0434\n",
            "Epoch 7 Batch 2000 Loss 0.0671\n",
            "Epoch 7 Batch 2100 Loss 0.0613\n",
            "Epoch 7 Batch 2200 Loss 0.0554\n",
            "Epoch 7 Batch 2300 Loss 0.0671\n",
            "Epoch 7 Batch 2400 Loss 0.0379\n",
            "Epoch 7 Batch 2500 Loss 0.0617\n",
            "Epoch 7 Batch 2600 Loss 0.0506\n",
            "Epoch 7 Batch 2700 Loss 0.0622\n",
            "Epoch 7 Batch 2800 Loss 0.0505\n",
            "Epoch 7 Batch 2900 Loss 0.0476\n",
            "Epoch 7 Loss 0.0585\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0422\n",
            "Epoch 8 Batch 100 Loss 0.0473\n",
            "Epoch 8 Batch 200 Loss 0.0563\n",
            "Epoch 8 Batch 300 Loss 0.0507\n",
            "Epoch 8 Batch 400 Loss 0.0493\n",
            "Epoch 8 Batch 500 Loss 0.0647\n",
            "Epoch 8 Batch 600 Loss 0.0462\n",
            "Epoch 8 Batch 700 Loss 0.0533\n",
            "Epoch 8 Batch 800 Loss 0.0549\n",
            "Epoch 8 Batch 900 Loss 0.0567\n",
            "Epoch 8 Batch 1000 Loss 0.0709\n",
            "Epoch 8 Batch 1100 Loss 0.0444\n",
            "Epoch 8 Batch 1200 Loss 0.0471\n",
            "Epoch 8 Batch 1300 Loss 0.0509\n",
            "Epoch 8 Batch 1400 Loss 0.0425\n",
            "Epoch 8 Batch 1500 Loss 0.0370\n",
            "Epoch 8 Batch 1600 Loss 0.0702\n",
            "Epoch 8 Batch 1700 Loss 0.0416\n",
            "Epoch 8 Batch 1800 Loss 0.0445\n",
            "Epoch 8 Batch 1900 Loss 0.0291\n",
            "Epoch 8 Batch 2000 Loss 0.0443\n",
            "Epoch 8 Batch 2100 Loss 0.0484\n",
            "Epoch 8 Batch 2200 Loss 0.0374\n",
            "Epoch 8 Batch 2300 Loss 0.0463\n",
            "Epoch 8 Batch 2400 Loss 0.0422\n",
            "Epoch 8 Batch 2500 Loss 0.0499\n",
            "Epoch 8 Batch 2600 Loss 0.0417\n",
            "Epoch 8 Batch 2700 Loss 0.0493\n",
            "Epoch 8 Batch 2800 Loss 0.0465\n",
            "Epoch 8 Batch 2900 Loss 0.0461\n",
            "Epoch 8 Loss 0.0504\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/ResearchAssistantModels'\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "print(\"üìÅ Folder ready at:\", folder_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ2bXDqss7Rx",
        "outputId": "61c9924f-45d5-4c46-ab47-09bec1d5d7a8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "üìÅ Folder ready at: /content/drive/MyDrive/ResearchAssistantModels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_path = f'{folder_path}/encoder.weights.h5'\n",
        "encoder.save_weights(encoder_path)\n",
        "print('encoder_svaed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfnMa9PJui3D",
        "outputId": "c447cc76-de1d-41e9-df62-609fd0073749"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_svaed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_path = f'{folder_path}/decoder.weights.h5'\n",
        "decoder.save_weights(decoder_path)"
      ],
      "metadata": {
        "id": "JfdNa4YHuyv0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_path = f'{folder_path}/encoder.keras'\n",
        "encoder.save(encoder_path)\n",
        "print(\"‚úÖ Full encoder model saved at:\", encoder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAPWf-uSv2vy",
        "outputId": "bab75283-c5cf-4e8c-c096-8ceb1aa263fc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Full encoder model saved at: /content/drive/MyDrive/ResearchAssistantModels/encoder.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_path = f'{folder_path}/decoder.keras'\n",
        "decoder.save(decoder_path)\n",
        "print(\"‚úÖ Full encoder model saved at:\", decoder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS4wSFYrv27c",
        "outputId": "f0afb080-fb0e-45ee-f402-2510adabeaac"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Full encoder model saved at: /content/drive/MyDrive/ResearchAssistantModels/decoder.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"([?.!,¬ø])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¬ø]+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "Bao4JgfMI6iH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_eng_len, max_fr_len))\n",
        "\n",
        "    # Preprocess the input sentence (same as training)\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [fr_tokenizer.word_index.get(i, fr_tokenizer.word_index['<unk>']) for i in sentence.split(' ')]\n",
        "    inputs = pad_sequences([inputs], maxlen=max_fr_len, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = (tf.zeros((1, units)), tf.zeros((1, units)))\n",
        "    enc_out, enc_h, enc_c = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = (enc_h, enc_c)\n",
        "    dec_input = tf.expand_dims([eng_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_eng_len):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        # store attention weights for visualization\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        # get predicted word\n",
        "        predicted_word = eng_tokenizer.index_word.get(predicted_id, '<unk>')\n",
        "\n",
        "\n",
        "        # if end token predicted ‚Üí stop translation\n",
        "        if predicted_word == '<end>':\n",
        "            return result.strip(), sentence, attention_plot\n",
        "        result += predicted_word + ' '\n",
        "        # predicted word becomes next input\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence, attention_plot\n"
      ],
      "metadata": {
        "id": "DIQuFaiuYjIZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_sentence = \"Tu ferais mieux de me dire exactement ce qu‚Äôil s‚Äôest pass√©.\"\n",
        "result = evaluate(french_sentence)\n",
        "print(result[0])"
      ],
      "metadata": {
        "id": "DXM90VNTs_kP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60bada9-ef56-4432-bf01-ddec09f02bf8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you'd better tell me exactly what it is <unk> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.ticker as ticker\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    attention = attention[:len(predicted_sentence), :len(sentence)]\n",
        "\n",
        "    cax = ax.matshow(attention, cmap='viridis')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence)\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "b8EFVsEZuu35"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result, sentence, attention_plot = evaluate(french_sentence)\n",
        "plot_attention(attention_plot, sentence.split(' '), result.split(\" \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "mLeVZRHyHAuR",
        "outputId": "9d311f7f-d2a0-4413-97e3-076cf53d91c8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1914731185.py:11: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels([''] + sentence, rotation=90)\n",
            "/tmp/ipython-input-1914731185.py:12: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_yticklabels([''] + predicted_sentence)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "reference = [['I', 'am', 'going', 'home']]\n",
        "candidate = ['I', 'am', 'going', 'to', 'home']\n",
        "\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWWc-v5zHk0A",
        "outputId": "5c96b017-812f-4813-efe6-47a05f62c9d3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.380245217279165e-78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "references = [\n",
        "    [['I', 'am', 'going', 'home']],\n",
        "    [['He', 'is', 'sleeping']]\n",
        "]\n",
        "candidates = [\n",
        "    ['I', 'am', 'going', 'home'],\n",
        "    ['He', 'sleeping']\n",
        "]\n",
        "\n",
        "score = corpus_bleu(references, candidates)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVoQnYvCN5eA",
        "outputId": "8818acad-5933-4bbb-9f11-cefc605d0055"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5985529678206387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"je suis fatigu√© .\",\n",
        "    \"il fait chaud aujourd'hui .\",\n",
        "    \"j'aime le football .\"\n",
        "]\n",
        "\n",
        "for sent in test_sentences:\n",
        "    predicted, _, _ = evaluate(sent)\n",
        "    print(f\"\\nInput: {sent}\")\n",
        "    print(f\"Predicted: {predicted}\")\n",
        "\n",
        "    # prepare reference and candidate tokens\n",
        "    reference = [sent.split()]  # human (French)\n",
        "    candidate = predicted.lower().replace('.', '').split()  # model (English)\n",
        "\n",
        "    bleu = sentence_bleu(reference, candidate)\n",
        "    print(f\"BLEU Score: {bleu:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFkkMApROGek",
        "outputId": "170f2012-d02f-48c5-d8eb-17fa85923a5d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: je suis fatigu√© .\n",
            "Predicted: i'm being <unk> .\n",
            "BLEU Score: 0.0000\n",
            "\n",
            "Input: il fait chaud aujourd'hui .\n",
            "Predicted: it's warm today .\n",
            "BLEU Score: 0.0000\n",
            "\n",
            "Input: j'aime le football .\n",
            "Predicted: i like soccer .\n",
            "BLEU Score: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "\n",
        "smooth_fn = SmoothingFunction().method1\n",
        "\n",
        "def evaluate_bleu_on_test(test_ds, num_samples=100):\n",
        "    total_bleu = 0\n",
        "    count = 0\n",
        "\n",
        "    for batch, (inp, targ) in enumerate(test_ds.take(num_samples)):\n",
        "        # Convert tensor to numpy/int32\n",
        "        inp = inp.numpy()\n",
        "        targ = targ.numpy()\n",
        "\n",
        "        # For each sentence in batch\n",
        "        for i in range(inp.shape[0]):\n",
        "            # Convert input tensor back to French sentence\n",
        "            inp_sentence = ' '.join([fr_tokenizer.index_word.get(idx, '') for idx in inp[i] if idx != 0])\n",
        "\n",
        "            # Convert target tensor back to English reference\n",
        "            reference = [eng_tokenizer.index_word.get(idx, '') for idx in targ[i] if idx not in [0]]\n",
        "            reference = [w for w in reference if w not in ['<start>', '<end>']]  # remove special tokens\n",
        "            reference = [reference]  # wrap inside a list (required by nltk)\n",
        "\n",
        "            # Predict translation using your model\n",
        "            prediction, _, _ = evaluate(inp_sentence)\n",
        "            candidate = prediction.lower().split()\n",
        "\n",
        "            # Calculate BLEU score\n",
        "            bleu = sentence_bleu(reference, candidate, smoothing_function=smooth_fn)\n",
        "            total_bleu += bleu\n",
        "            count += 1\n",
        "\n",
        "        # Stop after num_samples sentences\n",
        "        if count >= num_samples:\n",
        "            break\n",
        "\n",
        "    avg_bleu = total_bleu / count\n",
        "    print(f\"\\nAverage BLEU score over {count} samples: {avg_bleu:.4f}\")\n",
        "    return avg_bleu\n"
      ],
      "metadata": {
        "id": "x1NHgQ71PV9W"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_bleu = evaluate_bleu_on_test(test_ds, num_samples=100)\n",
        "print(\"Final BLEU Score:\", avg_bleu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ0Nf-9OQbiJ",
        "outputId": "3daa60d2-afb1-4391-d043-cd6035d4cabb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average BLEU score over 128 samples: 0.4943\n",
            "Final BLEU Score: 0.4942552932843735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1n6VXwyiQg22"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}